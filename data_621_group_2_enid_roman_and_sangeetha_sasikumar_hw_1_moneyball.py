# -*- coding: utf-8 -*-
"""Data 621 - Group 2 - Enid Roman and Sangeetha Sasikumar - HW 1 - Moneyball.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r2hj58Cs_NrNyx-HZ2v3eDWzyHvW0AHE

Please note we worked using R in colab so we could work as a group. My R broke down in the middle of the work transfering. Sorry for the delay. We have technical difficulties.

#**INTRODUCTION**

Data science and artificial intelligence are already making a great impact across diverse industries. Sports are no exception. Data science can help in various ways: Help captains make the right decisions, predict final score, deeper analysis of match performances, and patterns.

Data analytics was not new in baseball, since 1960s data was available, and everyone used it to do analysis. However, no one ever believes in its result.

The Moneyball dataset is associated with the book “Moneyball” by Michael Lewis. It tells the story of how the Oakland Athletics, a low-budget baseball team, used data-driven analysis and statistics to build a competitive team. The dataset contains player statistics and performance data that helped the A’s identify undervalued players with a high on base percentage. This approach revolutionized baseball by emphasizing data-driven decision-making, leading to the team’s success in the 2022 season and the broader adoption.

Moneyball succeeded for the Oakland A’s not only because of data analytics but also because of Beane’s out of the box thinking ability, the leader who understood and believed in the players and Peter Brand the Economist’s.

Reference: https://capablemachine.wordpress.com/2020/10/27/moneyball-data-science/

#**DATA EXPLORATION**

###**LIBRARY NEEDED**
"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext rpy2.ipython

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Install the libraries.
# install.packages("gridExtra")
# library(tidyverse)
# install.packages("ggplot2")
# library(ggplot2)
# install.packages("plotly")
# library(plotly)
# library(dplyr)
# library(tidyr)
# library(infer)
# library(forcats)
# library(DT)
# library(caret)
# library(ggcorrplot)
# install.packages("corrplot")
# devtools::install_github('taiyun/corrplot', build_vignettes = TRUE)
# library(corrplot)
# library(RColorBrewer)
# install.packages("MASS")
# library(MASS)
# install.packages("car")

"""###**ABOUT THE DATA SET**

This data set contains approximately 2200 records. Each record represents a professional baseball team from the years 1871 to 2006 inclusive. Each record has the performance of the team for the given year, with all of the statistics adjusted to match the performance of
a 162 game season.
"""

#%%R
#install.packages("imager")
#library(imager)
#im=load.image("https://github.com/Sangeetha-007/Moneyball/blob/6c416e4275bb32f07b5bf28fc38fac1e4feb0a30/moneyballdata.png")
#plot(im)
#im = load.image("C:/Users/enidr/OneDrive/Documents/CUNY SPS DATA 621/Data 621 HW 1/Pic HW1.png")
#plot(im)

"""[link text]("https://drive.google.com/file/d/1s3L3DQptlKbdCkNB0VdwATXHJmlL8aEc/view?usp=sharing"![moneyballdata.png](data:image))"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# test <- read.csv ('https://raw.githubusercontent.com/Sangeetha-007/Moneyball/main/moneyball-evaluation-data%20(3).csv')
# train <- read.csv ('https://raw.githubusercontent.com/Sangeetha-007/Moneyball/main/moneyball-training-data%20(1).csv')

# Commented out IPython magic to ensure Python compatibility.
# %%R
# head(train)

"""For each of the fields in the dataset, the higher the value is, the more likely for a win. For example, teams that can consistently get base hits tend to score more runs, which can lead to more wins. The mean/average of base hits from 1871 to 2006 is 1469.39 hits."""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# mean(train$TEAM_BATTING_H)
#

"""###**SUMMARY STATS**

We gathered summary statistics for our dataset to gain a deeper understanding of the data before commencing the modeling process.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# summary(train)



"""We first noticed missing values (NA's) in the dataset. At a quick look, it's clear that, on average, a team wins around 81 games per season, which is half of a typical MLB season's total games. Additionally, over a 162-game season, batters usually get about 9 base hits per game, pitchers issue around 4 walks per game, and pitchers record roughly 5 strikeouts per game.






"""

# Commented out IPython magic to ensure Python compatibility.
# # Drop the INDEX column - this won't be useful
# %%R
# train <- train %>%
#   dplyr::select(-INDEX)

"""###**DISTRIBUTION**

Here our objective was to grasp the distribution profiles pertaining to each individual variable.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# # Prepare data for ggplot
# gather_train <- train %>%
#   gather(key = 'variable', value = 'value')
# 
# # Histogram plots of each variable
# ggplot(gather_train) +
#   geom_histogram(aes(x=value, y = ..density..), bins=30) +
#   geom_density(aes(x=value), color='blue') +
#   facet_wrap(. ~variable, scales='free', ncol=4)

"""The distribution profiles reveal instances of kurtosis, specifically right skewness, in the variables BASERUN_CS, BASERUN_SB, FIELDING_E, PITCHING_BB, PITCHING_H, and PITCHING_SO. These deviations from a typical normal distribution can pose challenges to the assumptions of linear regression, potentially necessitating data transformations. Additionally, BATTING_HBP, BATTING_HR, PITCHING_HR, and BATTING_SO exhibit a bimodal pattern. Bimodality in the data is both intriguing and problematic, suggesting the presence of two distinct groups or classes within the baseball season data. These seasons seem to have produced either higher or lower values for the bimodal variables, presenting an area for further investigation and exploration.

###**BOX PLOT**

In addition to constructing histogram distributions, we chose to utilize box plots as a means of assessing the variability of each variable.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# # Prepare data for ggplot
# gather_train <- train %>%
#   gather(key = 'variable', value = 'value')
# 
# # Boxplots for each variable
# ggplot(gather_train, aes(variable, value)) +
#   geom_boxplot() +
#   facet_wrap(. ~variable, scales='free', ncol=6)

"""The box plots reveal significant outliers and a consistent presence of uniform (primarily zero) values among our predictor variables. Managing these outliers may involve imputation as needed, and sparse datasets may be subject to removal as deemed suitable.

###**VARIABLE**

We aimed to produce scatter plots illustrating the connections between each variable and the target variable, TARGET_WINS, to gain insights into their interrelationships.

####**SCATTER PLOT**

Scatter plots visually reveal relationships, patterns, and outliers between two continuous variables, aiding in understanding, model assessment, and data exploration in data analysis and statistics. Initially, the graphs were all in one plot, however the graphs were not clearly visible.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# library(ggplot2)
# library(gridExtra)
# 
# # Create scatter plots for selected fields vs. TARGET_WINS
# scatterplot1 <- ggplot(data = train, aes(x = TEAM_BATTING_H, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_BATTING_H", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_BATTING_H vs. TARGET_WINS")
# 
#

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot2 <- ggplot(data = train, aes(x = TEAM_BATTING_2B, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_BATTING_2B", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_BATTING_2B vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot3 <- ggplot(data = train, aes(x = TEAM_BATTING_3B, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_BATTING_3B", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_BATTING_3B vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot4 <- ggplot(data = train, aes(x = TEAM_BATTING_HR, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_BATTING_HR", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_BATTING_HR vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot5 <- ggplot(data = train, aes(x = TEAM_BATTING_BB, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_BATTING_BB", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_BATTING_BB vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot6 <- ggplot(data = train, aes(x = TEAM_BATTING_SO, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_BATTING_SO", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_BATTING_SO vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot7 <- ggplot(data = train, aes(x = TEAM_BASERUN_SB, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_BATTING_SB", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_BATTING_SB vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot8 <- ggplot(data = train, aes(x = TEAM_BASERUN_CS, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_BASERUN_CS", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_BASERUN_CS vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot9 <- ggplot(data = train, aes(x = TEAM_BATTING_HBP, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_BATTING_HBP", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_BATTING_HBP vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot10 <- ggplot(data = train, aes(x = TEAM_PITCHING_H, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_PITCHING_H", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_PITCHING_H vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot11 <- ggplot(data = train, aes(x = TEAM_PITCHING_HR, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_PITCHING_HR", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_PITCHING_HR vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot12 <- ggplot(data = train, aes(x = TEAM_PITCHING_BB, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_PITCHING_BB", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_PITCHING_BB vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot13 <- ggplot(data = train, aes(x = TEAM_PITCHING_SO, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_PITCHING_SO", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_PITCHING_SO vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot14 <- ggplot(data = train, aes(x = TEAM_FIELDING_E, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_FIELDING_E", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_FIELDING_E vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# scatterplot15 <- ggplot(data = train, aes(x = TEAM_FIELDING_DP, y = TARGET_WINS)) +
#   geom_point() +
#   labs(x = "TEAM_FIELDING_DP", y = "TARGET_WINS") +
#   ggtitle("Scatter Plot of TEAM_FIELDING_DP vs. TARGET_WINS")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Create more scatter plots for other selected fields
# 
# # Arrange the scatter plots in a 2x2 grid
# grid.arrange(scatterplot1, scatterplot2, scatterplot3, scatterplot4, ncol = 2)
# grid.arrange(scatterplot5, scatterplot6, scatterplot7, scatterplot8, ncol = 2)
# grid.arrange(scatterplot9, scatterplot10, scatterplot11, scatterplot12, ncol = 2)
# grid.arrange(scatterplot13, scatterplot14, scatterplot15, ncol = 2)

"""The plots show that hitting more doubles or home runs generally leads to more wins.

However, there are big problems with our data:

* Many variables aren't normally distributed and
need fixing.
* We have lots of missing data that needs filling or removing.
* Some data is missing but marked as 0, and there are strange outliers.
* For example, it's unlikely a team had 0 wins or hit 0 home runs in a season. Pitching stats have odd outliers too, like a team supposedly getting 20,000 strikeouts (impossible in baseball).
* The error variable also doesn't make sense; it suggests an unusual change in error frequency over time.

###**MISSING DATA**

When we first looked at the raw data, we noticed some missing values. Now, let's identify which fields contain these missing values.
"""

# Commented out IPython magic to ensure Python compatibility.
# # Identify missing data by Feature and display percent breakout
# %%R
# missing <- colSums(train %>% sapply(is.na))
# missing_pct <- round(missing / nrow(train) * 100, 2)
# stack(sort(missing_pct, decreasing = TRUE))

# Commented out IPython magic to ensure Python compatibility.
# %%R
# sorted_missing_pct <- sort(missing_pct, decreasing = TRUE)
# 
# # Create a df for the stacked bar chart
# missing_data <- data.frame(column = names(sorted_missing_pct), missing_pct = sorted_missing_pct)
# 
# 
# 
# ggplot(missing_data, aes(x = reorder(column, missing_pct), y = missing_pct, fill = column)) +
#   geom_bar(stat = "identity") +
#   labs(title = "Missing Values by Column",
#        x = "Columns",
#        y = "Percentage of Missing Values") +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
#   scale_fill_brewer(palette = "Set3")

"""We found that about 91.6% of rows don't have data in the "TEAM_BATTING_HBP" column, so we'll remove that column. The columns "TEAM_BASERUN_CS" (caught stealing) and "TEAM_BASERUN_SB" (stolen bases) have some missing data. Stolen bases weren't officially recorded until 1887 in baseball history, so some of the missing data could be from 1871-1886. We'll add this missing information. Also, there are many missing values in "TEAM_BATTING_SO" (batter strikeouts) and "TEAM_PITCHING_SO" (pitcher strikeouts), which seems unusual. We'll fill in these gaps using the median value for each respective feature."""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Drop the TEAM_BATTING_HBP field
# #train <- train %>%
#   #select(TEAM_BATTING_HBP)
# train_drop_col <- train %>% select(-c(TEAM_BATTING_HBP))

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Check to see that TEAM_BATTING_HBP was dropped.
# print(names(train_drop_col))
# str(train_drop_col)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Calculated the mean of all columns and created a colMeans column to hold the means in order to replace the NA with colums mean.
#  (cmeans <- colMeans(train_drop_col, na.rm=TRUE))

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Replace NAs with mean.
# 
# # Assuming 'train_drop_col' is your original data frame.
# clean_train <- train_drop_col
# 
# # Specify the column names you want to fill with means.
# columns_to_fill <- c("TEAM_BASERUN_CS", "TEAM_FIELDING_DP", "TEAM_BASERUN_SB", "TEAM_BATTING_SO", "TEAM_PITCHING_SO")
# 
# # Loop through column names and fill missing values with means
# for (col_name in columns_to_fill) {
#   clean_train[is.na(train_drop_col[, col_name]), col_name] <- cmeans[col_name]
# }

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Check to see there is no NAs
# summary(clean_train)

"""###**TARGETING CORRELATION**

Now that we've filled in the missing data, we can use the scatter plots we created earlier to see how our target variable relates to the other variables. We'll focus on the ones that show strong positive or negative connections. Variables with correlations close to zero likely won't help explain a team's wins.
"""

# Commented out IPython magic to ensure Python compatibility.
# # Show feature correlations/target by decreasing correlation
# %%R
# stack(sort(cor(clean_train[,1], clean_train[,2:ncol(clean_train)])[,], decreasing=TRUE))

""""TEAM_BATTING_H" and "TEAM_BATTING_2B" are strongly positively correlated with "TEAM_TOTAL_WINS." This is expected because more hits often lead to more runs and a greater likelihood of winning. On the other hand, the other variables show weak or slightly negative correlations, indicating they may not be strong predictors of wins.

###**MULTICOLLINEARITY**

One issue that can arise in multi-variable regression is the presence of correlations between variables, known as multicollinearity. You can perform a quick check for this by calculating correlations between the variables.
"""

# Commented out IPython magic to ensure Python compatibility.
# # Calculate and plot the Multicolinearity
# %%R
# 
# corplot_train <- cor(clean_train, method = "pearson")
# col_gd <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
# 
# install.packages("corrplot")
# library(ggcorrplot)
# # Create the correlation plot
# corrplot(corplot_train, method = "color", col = col_gd(200),
#          type = "upper", order = "hclust",
#          addCoef.col = "Black",
#          tl.col = "black", tl.srt = 45, number.cex = 0.5, tl.cex = 0.8)

"""We notice that some variables are closely related, like PITCHING_BB and BATTING_BB. When selecting features for our models, we must consider these relationships and avoid including pairs with strong correlations.

This dataset poses a challenge because many predictive features are interconnected. Stronger teams tend to show both positive and negative features moving in opposite directions. Additionally, many features naturally go hand-in-hand; for example, an increase in batter strikeouts often leads to a decrease in hit-related metrics, and more Base Hits tend to correlate with higher values in the 2B, 3B, and 4B columns, among others.

#**DATA PREPARATION**

Summary: In our data exploration and preparation, we took several actions to improve the quality of our dataset:

* Removal of Fields:

We removed the "TEAM_BATTING_HBP" field because it had over 90% missing data, making it less informative for analysis.

The "INDEX" field was also removed as it didn't provide any valuable information for modeling.

* Handling Missing Values:

Missing values were identified in several fields, including "TEAM_BASERUN_CS," "TEAM_FIELDING_DP," "TEAM_BASERUN_SB," "TEAM_BATTING_SO," and "TEAM_PITCHING_SO."

To address this, we replaced the missing values in these fields with their respective median values. This decision was based on the assumption that it's highly unlikely that teams had none of these occurrences during an entire season.

* Handling Unreasonable Outliers:

Unreasonable outliers were detected in certain fields, such as "TEAM_PITCHING_SO" (pitching strikeouts), "TEAM_PITCHING_H" (hits allowed per game), "TEAM_PITCHING_BB" (walks), and "TEAM_FIELDING_E" (fielding errors).

To address this issue, we set limits for these outliers. For example, values exceeding 4000 for "TEAM_PITCHING_SO" (equivalent to 25 strikeouts per game) were considered outliers and replaced with the median.

Similar limits were applied to "TEAM_PITCHING_H," "TEAM_PITCHING_BB," and "TEAM_FIELDING_E" to ensure that extreme values that exceeded what is reasonable or possible for standard game length were handled appropriately.

These preprocessing steps were crucial in ensuring the quality and reliability of our dataset for subsequent analysis and modeling.

###**TRANSFORM NON-NORMAL VARIABLES**

Based on our analysis of the above histogram plots from our observation, it is evident that certain variables in our dataset exhibit significant skewness. In response to this observation, we opted to apply various transformations aimed at achieving a more normal distribution. Below, we present visual representations that illustrate the alterations in the distributions both before and after these transformations:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # created empty data frame to store transformed variables
# clean_temp <- data.frame(matrix(ncol = 1, nrow = length(clean_train$TEAM_TARGET_WINS)))

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # created empty data frame to store transformed variables
# #clean_temp <- data.frame(matrix(ncol = 1, nrow = length(clean_train$TEAM_TARGET_WINS)))
# 
# # performed boxcox transformation after identifying proper lambda
# clean_temp$TEAM_BATTING_3B <- clean_train$TEAM_BATTING_3B
# teambatting3b_lambda <- BoxCox.lambda(clean_train$TEAM_BATTING_3B)
# clean_temp$TEAM_BATTING_3B_transform <- log(clean_train$TEAM_BATTING_3B)
# 
# # performed boxcox transformation after identifying proper lambda
# clean_temp$TEAM_BATTING_HR <- clean_train$TEAM_BATTING_HR
# teambattingHR_lambda <- BoxCox.lambda(clean_train$TEAM_BATTING_HR)
# clean_temp$TEAM_BATTING_HR_transform <- BoxCox(clean_train$TEAM_BATTING_HR, battingHR_lambda)
# 
# # performed a log transformation
# clean_temp$TEAM_PITCHING_BB <- clean_train$TEAM_PITCHING_BB
# clean_temp$TEAM_PITCHING_BB_transform <- log(clean_train$PITCHING_BB)
# 
# # performed a log transformation
# clean_temp$TEAM_PITCHING_SO <- clean_train$TEAM_PITCHING_SO
# clean_temp$TEAM_PITCHING_SO_transform <- log(clean_train$TEAM_PITCHING_SO)
# 
# # performed an inverse log transformation
# clean_temp$TEAM_FIELDING_E <- clean_train$TEAM_FIELDING_E
# clean_temp$TEAM_FIELDING_E_transform <- 1/log(clean_train$TEAM_FIELDING_E)
# 
# # performed a log transformation
# clean_temp$TEAN_BASERUN_SB <- clean_train$TEAM_BASERUN_SB
# clean_temp$TEAM_BASERUN_SB_transform <- log(clean_train$TEAM_BASERUN_SB)
# 
# clean_temp <- clean_temp[, 2:13]
# 
# histbox(clean_temp, c(6, 2))

"""###**FINALIZATION OF DATASET FOR MODEL BUILDING**

Now that we have finished applying our transformations, we can incorporate these changes into our "clean_train" dataframe and proceed with the construction of our models.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Build clean dataframe with transformation
# clean_train <- data.frame(cbind(clean_train,
#                       TEAM_BATTING_3B_transform = clean_temp$TEAM_BATTING_3B_transform,
#                       TEAM_BATTING_HR_transform = clean_temp$TEAM_BATTING_HR_transform,
#                       TEAM_BASERUN_SB_transform = clean_temp$TEAM_BASERUN_SB_transform,
#                       TEAM_PITCHING_BB_transform = clean_temp$TEAM_PITCHING_BB_transform,
#                       TEAM_PITCHING_SO_transform = clean_temp$TEAM_PITCHING_SO_transform,
#                       TEAM_FIELDING_E_transform = clean_temp$TEAM_FIELDING_E_transform))
# 
# is.na(clean_train) <- sapply(clean_train, is.infinite)
# 
# # Impute missing value with the mean
# mean = mean(clean_train$TEAM_BATTING_3B_transform, na.rm = TRUE)
# mean2 = mean(clean_train$TEAM_BASERUN_SB_transform, na.rm = TRUE)
# mean3 = mean(clean_train$TEAM_PITCHING_SO_transform, na.rm = TRUE)
# 
# clean_team$TEAM_BATTING_3B_transform[is.na(clean_team$TEAM_BATTING_3B_transform)] <- mean
# clean_team$TEAM_BASERUN_SB_transform[is.na(clean_team$TEAM_BASERUN_SB_transform)] <- mean2
# clean_team$TEAM_PITCHING_SO_transform[is.na(clean_team$TEAM_PITCHING_SO_transform)] <- mean3

# Commented out IPython magic to ensure Python compatibility.
# %%R
# #Sangeetha's version
# # Define clean_temp data frame
# clean_temp <- data.frame(
#   TEAM_BATTING_3B_transform = log(clean_train$TEAM_BATTING_3B),
#   TEAM_BATTING_HR_transform = log(clean_train$TEAM_BATTING_HR),  # Use a log transformation
#   TEAM_PITCHING_BB_transform = log(clean_train$TEAM_PITCHING_BB),
#   TEAM_PITCHING_SO_transform = log(clean_train$TEAM_PITCHING_SO),
#   TEAM_FIELDING_E_transform = 1/log(clean_train$TEAM_FIELDING_E),
#   TEAM_BASERUN_SB_transform = log(clean_train$TEAM_BASERUN_SB)
# )
# 
# # Combine transformations with clean_train
# clean_train <- data.frame(
#   clean_train,
#   clean_temp
# )
# 
# # Impute missing values with mean
# mean_3B <- mean(clean_train$TEAM_BATTING_3B_transform, na.rm = TRUE)
# mean_SB <- mean(clean_train$TEAM_BASERUN_SB_transform, na.rm = TRUE)
# mean_SO <- mean(clean_train$TEAM_PITCHING_SO_transform, na.rm = TRUE)
# 
# clean_train$TEAM_BATTING_3B_transform[is.na(clean_train$TEAM_BATTING_3B_transform)] <- mean_3B
# clean_train$TEAM_BASERUN_SB_transform[is.na(clean_train$TEAM_BASERUN_SB_transform)] <- mean_SB
# clean_train$TEAM_PITCHING_SO_transform[is.na(clean_train$TEAM_PITCHING_SO_transform)] <- mean_SO

"""Having finished our transformations, we can now incorporate these changes into our clean_df dataframe and proceed with the construction of our models.

#**BUILD MODELS**

After cleaning our data and understanding our dataset better, we're ready to create multiple linear regression models.

First, we split our dataset into a training set (80%) and a testing set (20%). In Model #1, we build a linear regression model using all the non-transformed features that we didn't remove during data cleaning.

In Model #2, we use the step AIC function in R for stepwise feature selection to reduce the number of predictors and handle multicollinearity issues from Model #1.

Model #3 includes all the cleaned data but also incorporates some transformed features instead of their original versions. We again apply stepwise selection to find the best feature set and reduce multicollinearity.

These models help us analyze and predict outcomes based on our dataset.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# install.packages("rsample")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# library(rsample)
# # Splitting the dataset into train/testing set with 80/20 split
# set.seed(3)
# df_split <- initial_split(clean_train, prop = 0.8)
# df_train <- training(df_split)
# df_test <- testing(df_split)
# 
# #head(df_test)
#

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Train a multiple regression model
# model <- lm(TARGET_WINS ~ ., data = df_train)
# 
# # Print the summary of the model
# print(summary(model))
# 
# # Make predictions on the test set
# predictions <- predict(model, newdata = df_test)
# 
# # Evaluate the model (you can use different metrics depending on your preference)
# mse <- mean((df_test$TARGET_WINS - predictions)^2)
# rmse <- sqrt(mse)
# mae <- mean(abs(df_test$TARGET_WINS - predictions))
# 
# # Print the evaluation metrics
# cat("Mean Squared Error (MSE):", mse, "\n")
# cat("Root Mean Squared Error (RMSE):", rmse, "\n")
# cat("Mean Absolute Error (MAE):", mae, "\n")
# 
#

"""In multiple regression, the t-value associated with a predictor variable measures whether that variable has a statistically significant impact on the dependent variable while considering the influence of other predictors. Smaller t-values indicate that the predictor's contribution to the model is less likely to be significant when accounting for the effects of other predictors. Next I am going to make a linear model using the top 2 t-values."""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Train a multiple regression model
# model <- lm(TARGET_WINS ~ TEAM_BATTING_H+ TEAM_BASERUN_SB, data = df_train)
# 
# # Print the summary of the model
# print(summary(model))
# 
# # Make predictions on the test set
# predictions <- predict(model, newdata = df_test)
# 
# # Evaluate the model (you can use different metrics depending on your preference)
# mse <- mean((df_test$TARGET_WINS - predictions)^2)
# rmse <- sqrt(mse)
# mae <- mean(abs(df_test$TARGET_WINS - predictions))
# 
# # Print the evaluation metrics
# cat("Mean Squared Error (MSE):", mse, "\n")
# cat("Root Mean Squared Error (RMSE):", rmse, "\n")
# cat("Mean Absolute Error (MAE):", mae, "\n")

"""#**SELECT MODEL**

To determine which of the two models (Model 1 and Model 2) is better, we can consider several factors and metrics:

Model 1:
•	Multiple R-squared (R²): 0.3298
•	Adjusted R-squared (Adjusted R²): 0.3246
•	Residual standard error: 12.98
•	Mean Squared Error (MSE): 177.3539
•	Root Mean Squared Error (RMSE): 13.31743
•	Mean Absolute Error (MAE): 10.28263

Model 2:
•	Multiple R-squared (R²): 0.1635
•	Adjusted R-squared (Adjusted R²): 0.1626
•	Residual standard error: 14.46
•	Mean Squared Error (MSE): 210.4669
•	Root Mean Squared Error (RMSE): 14.50748
•	Mean Absolute Error (MAE): 11.1367

Here's a comparison of the models:

•	R-squared (R²): Model 1 has a higher R-squared value (0.3298) compared to Model 2 (0.1635). This suggests that Model 1 explains a larger proportion of the variance in the target variable.
•	Adjusted R-squared (Adjusted R²): Model 1 also has a higher adjusted R-squared value (0.3246) compared to Model 2 (0.1626). Adjusted R-squared takes into account the number of predictors in the model and penalizes for excessive complexity.
•	Residual standard error: Model 1 has a lower residual standard error (12.98) compared to Model 2 (14.46). A lower residual standard error indicates that Model 1 provides a better fit to the data.
•	Mean Squared Error (MSE): Model 1 has a lower MSE (177.3539) compared to Model 2 (210.4669). A lower MSE indicates that Model 1's predictions are closer to the actual values on average.
•	Root Mean Squared Error (RMSE): Model 1 has a lower RMSE (13.31743) compared to Model 2 (14.50748). A lower RMSE suggests that Model 1's predictions have smaller errors.
•	Mean Absolute Error (MAE): Model 1 has a lower MAE (10.28263) compared to Model 2 (11.1367). A lower MAE indicates that Model 1's predictions have smaller absolute errors.

Based on these metrics, Model 1 appears to be the better model. It has higher R-squared values, lower residual standard error, and lower error metrics (MSE, RMSE, and MAE), all of which indicate a better fit to the data and better predictive performance.
"""